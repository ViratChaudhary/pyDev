# -*- coding: utf-8 -*-
"""Regression_Analysis_Insurance_TensorFlow

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10DIElujjUAkf5OuU1ykB-VlVGFL-RMX2
"""

# Import the modules

import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
insurance = pd.read_csv('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv')
insurance

# Numerical encoding - One-Hot encoding
insurance_one_hot = pd.get_dummies(insurance)
insurance_one_hot.head()

# Create the features and labels (X and y)

X = insurance_one_hot.drop('charges', axis = 1)
y = insurance_one_hot['charges']

X.head()

y.head()

from sklearn.model_selection import train_test_split

# Split the dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
len(X_train), len(X_test), len(y_train), len(y_test)

# Build the neural network

tf.random.set_seed(42)

insurance_model = tf.keras.Sequential([
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])

insurance_model.compile(loss=tf.keras.losses.mean_absolute_error,
                        optimizer=tf.keras.optimizers.SGD(),
                        metrics = ['mae'])

insurance_model.fit(X_train, y_train, epochs = 100)

insurance_model.evaluate(X_test, y_test)

y_train.median(), y_train.mean() # Model is substantially inaccurate

# Improve the model | Add an extra layer with more hidden units 

tf.random.set_seed(42)

insuranceModel2 = tf.keras.Sequential([
    tf.keras.layers.Dense(100),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)                                          
])

insuranceModel2.compile(loss = tf.keras.losses.mean_absolute_error,
                          optimizer = tf.keras.optimizers.Adam(),
                          metrics = ['mae'])

insuranceModel2.fit(X_train, y_train, epochs = 100, verbose = 1)

insuranceModel2.evaluate(X_test, y_test)

# Improve the model | Train for longer

tf.random.set_seed(42)

insuranceModel3 = tf.keras.Sequential([
    tf.keras.layers.Dense(100),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])

insuranceModel3.compile(loss = tf.keras.losses.mean_absolute_error,
                        optimizer = tf.keras.optimizers.Adam(),
                        metrics = ['mae'])

history = insuranceModel3.fit(X_train, y_train, epochs = 200)

insuranceModel3.evaluate(X_test, y_test)

# Plot History (loss curve or training curve)_

pd.DataFrame(history.history).plot()
plt.ylabel("Loss")
plt.xlabel("Epochs")

"""# Preproccesing Data | Normalisation and Standardisation"""

X['age'].plot(kind='hist')

X['bmi'].plot(kind='hist')

X['children'].value_counts()

import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

insurance = pd.read_csv('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv')
insurance

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split

# Preprocessing the data (One-Hot Encode all the non-numerical columns and MinMaxScale (0-1) all the numerical columns)

# Create a column transformer
ct = make_column_transformer((MinMaxScaler(), ['age', 'bmi', 'children']),
                             (OneHotEncoder(handle_unknown='ignore'), ['sex', 'smoker', 'region']))

# Create X and y
X = insurance.drop("charges", axis = 1)
y = insurance['charges']

# Build train and test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

# Fit the column transformer to only the training data
ct.fit(X_train)

# Transform training and test data with normalisation (MinMaxScaler and OneHotEncoder)
X_train_normal = ct.transform(X_train)
X_test_normal = ct.transform(X_test)

X_train.loc[0]

X_train_normal[0].squeeze()

X_train.shape, X_train_normal.shape

"""So essentially, since there was string and numerical data, it must be preproccessed. In order to do that, the numerical columns have been normalised
to fit between [0, 1] as its domain and the string columns have been one hot encoded. This was done using a column transformer.

Once the column transformer has been created, it is fit only onto the training data ready for training set to be inputted into the neural network.
"""

# Build the Neural Network

tf.random.set_seed(42)

insuranceModel4 = tf.keras.Sequential([
    tf.keras.layers.Dense(100),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])

insuranceModel4.compile(loss = tf.keras.losses.mean_absolute_error,
                        optimizer = tf.keras.optimizers.Adam(),
                        metrics = ['mae'])

history = insuranceModel4.fit(X_train_normal, y_train, epochs = 200, verbose = 1)

insuranceModel4.evaluate(X_test_normal, y_test)

insuranceModel4.summary()

pd.DataFrame(history.history).plot()
plt.xlabel('Epochs')
plt.ylabel('Loss')