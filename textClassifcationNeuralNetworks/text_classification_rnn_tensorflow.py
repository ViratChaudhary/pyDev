# -*- coding: utf-8 -*-
"""Text_Classification_RNN_TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13R8r1z9LxPHNal9K3xHSXZeEbbEMYubq
"""

import numpy as np

import tensorflow_datasets as tfds
import tensorflow as tf

import matplotlib.pyplot as plt

trainData, testData = tfds.load(name = 'imdb_reviews', split = ['train', 'test'], batch_size = -1, as_supervised = True)

trainData

testData

trainFeatures, trainLabels = tfds.as_numpy(trainData)
testFeatures, testLabels = tfds.as_numpy(testData)

trainFeatures, trainLabels

trainFeatures

trainLabels

# Numerical Encoding using Text Vectorization 

encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens = 1000)
encoder.adapt(trainFeatures) # Sets the vocabulary for the given max_tokens for the training features

vocabulary = encoder.get_vocabulary()
vocabulary = np.array(vocabulary)
len(vocabulary), vocabulary.squeeze() # Shows the 1000 tokens all of which will have their own vector when embedded

# Binary Classification Model Architecture

# layer 1 - encoder (Text Vectorization) which converts the text into token indicies (array form)
# layer 2 - embedding layer which converts the token given from the encoder into sequences of vectors for training
# Layer ... - Bidirectional layer for past/present/future computation
# Dense Layer - For additional computation and pattern recognition
# Output Dense Layer - To determine classification

# Create the RNN model

classificationModel = tf.keras.Sequential([encoder,
                                          tf.keras.layers.Embedding(
                                                input_dim = len(encoder.get_vocabulary()),
                                                output_dim = 64,
                                                mask_zero = True),
                                          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
                                          tf.keras.layers.Dense(64, activation = 'relu'),
                                          tf.keras.layers.Dense(1)])

classificationModel.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True),
                            optimizer = tf.keras.optimizers.Adam(1e-4),
                            metrics = ['accuracy'])

history = classificationModel.fit(trainFeatures, trainLabels, epochs = 4)

loss, accuracy = classificationModel.evaluate(testFeatures, testLabels)

print('Test Loss: ', loss)
print('Test Accuracy: ', accuracy)

classificationModel.predict(np.array(['This was a good text']))

classificationModel.summary()

import pandas as pd

timestamp = pd.DataFrame(history.history) # Model Training timestamps
timestamp

# Visualise the Loss 

plt.figure(figsize = (10, 7))
plt.plot(timestamp['loss'])
plt.title('RNN Loss Function Over Training Period')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

# Visualise the Accuracy 

plt.figure(figsize = (10, 7))
plt.plot(timestamp['accuracy'])
plt.title('RNN Accuracy Function Over Training Period')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.show()

# Co-Evaluation of Accuracy and Loss

plt.figure(figsize = (10, 7))
plt.title('Co-Evaluation of Accuracy and Loss of RNN classificationModel')
plt.xlabel('Epochs')
plt.ylabel('Percentage')
plt.plot(pd.DataFrame(history.history));